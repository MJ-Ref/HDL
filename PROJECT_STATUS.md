# LPCA Project Status

**Last Updated:** January 14, 2026
**Current Phase:** Milestone 1 COMPLETE - Ready for M2 (Requires Cloud GPUs)

---

## Executive Summary

The LPCA (Latent-Path Communication for AI Agents) research project has completed **Milestone 0** (Foundation) and **Milestone 1** (Latent Baselines). Key finding: **raw latent communication does NOT work without training**. Text communication (P1) achieves 30% success vs 0% for both CIPHER (E0) and Activation Grafting (A0). Safety evaluation (E5) passed all metrics. The codebase is production-ready with 126 tests passing. **Next step: M2 Codec Training requires cloud GPUs.**

---

## Milestone Progress

### Milestone 0: Foundation COMPLETE

| Deliverable | Status | Location |
|-------------|--------|----------|
| Experiment configuration system | Done | `lpca/core/config.py` |
| Episode logging (JSONL + Parquet) | Done | `lpca/core/logging.py` |
| Pre-registered metrics | Done | `lpca/core/metrics.py` |
| Budget accounting | Done | `lpca/core/budget.py` |
| Split-info synthetic tasks (S1-S3) | Done | `lpca/envs/split_synthetic.py` |
| Text baselines (P0-P5) | Done | `lpca/channels/text.py` |
| Model wrapper with hooks | Done | `lpca/agents/model_wrapper.py` |
| Experiment runner | Done | `scripts/demo_experiment.py` |
| Unit tests | Done | `tests/` (77 tests passing) |

**Exit Criteria:** ALL MET
- P1 >> P0 confirmed (20% vs 0% in demo)
- Deterministic generation verified
- Metrics logging verified

---

### Milestone 1: Latent Baselines âœ… COMPLETE

| Deliverable | Status | Location |
|-------------|--------|----------|
| CIPHER expected embedding (E0) | âœ… Done | `lpca/channels/cipher.py` |
| Activation grafting (A0) | âœ… Done | `lpca/channels/activation.py` |
| LLM Agent with activation capture | âœ… Done | `lpca/agents/llm_agent.py` |
| Layer sweep configs | âœ… Done | `lpca/channels/activation.py` |
| Combine function sweep | âœ… Done | `lpca/channels/activation.py` |
| Analysis scripts | âœ… Done | `scripts/analysis/` |
| Statistical tests | âœ… Done | `scripts/analysis/statistical_tests.py` |
| Real model evaluation (E1) | âœ… Done | P1=30%, P0=0% |
| CIPHER evaluation (E3) | âœ… Done | E0=0% (negative result) |
| Activation grafting (E4) | âœ… Done | A0=0% at all layers (negative result) |
| Safety evaluation (E5) | âœ… Done | All metrics PASS |

**Exit Criteria:** âœ… ALL COMPLETE
- [x] P1 >> P0 confirmed (30% vs 0%, p < 0.05)
- [x] A0/E0 tested - no improvement over P0 (raw latent doesn't work)
- [x] Clear evidence that **trained** latent communication needed (M2/M3)
- [x] Safety evaluation passed all pre-committed thresholds

---

### Milestone 4: Safety Evaluation âœ… E5 COMPLETE

| Deliverable | Status | Location |
|-------------|--------|----------|
| Compliance gap tester | âœ… Done | `lpca/safety/compliance.py` |
| Linear probe monitors | âœ… Done | `lpca/safety/monitors.py` |
| Monitor ensemble | âœ… Done | `lpca/safety/monitors.py` |
| Covert channel probes | âœ… Done | `lpca/safety/monitors.py` |
| Stop conditions | âœ… Done | Pre-committed in all modules |
| E5 Evaluation Run | âœ… Done | All metrics PASS |
| Bloom integration | Pending | `lpca/safety/bloom_eval.py` |

**E5 Results:**
| Metric | Value | Threshold | Status |
|--------|-------|-----------|--------|
| Compliance Gap | 17.5% | â‰¤20% | âœ… OK |
| Monitor Disagreement | 2.65% | â‰¤50% | âœ… OK |
| Covert Channel Capacity | 8 bits | â‰¤10 bits | âœ… OK |

---

### Milestone 2-3: Codec Development â³ BLOCKED ON CLOUD GPUs

**Why M2 is needed:** M1 showed that raw latent channels don't work. Training is required to create semantically meaningful latent packets.

**Cloud GPU Requirements:**
| Resource | Minimum | Recommended |
|----------|---------|-------------|
| GPU | 1Ã— A100 40GB | 4Ã— A100 80GB |
| VRAM | 40GB | 80GB |
| Time | 40-80 GPU-hours | - |
| Cost | $50-100 | $100-200 |

**Recommended Providers:**
1. **Modal.com** â­ RECOMMENDED - $2.10/hr A100 40GB, $30/mo free credits, $10K research grants
2. **Lambda Labs** - $1.10/hr A100 40GB, reliable
3. **RunPod** - $0.74/hr A100, spot instances
4. **Vast.ai** - $0.50-1.00/hr, community market

**Why Modal is Recommended:**
- $30/month free credits (covers ~14 hours of A100 40GB)
- **$10,000 academic research grants available**
- Serverless - only pay for compute, no idle costs
- Python-native API, easy to integrate with existing code
- No GPU provisioning/setup hassle

**Why Local M3 Max is Insufficient:**
- Training requires full model gradients (~12GB memory)
- MPS doesn't support all training operations efficiently
- Would take 10-20Ã— longer than cloud A100

---

## Recent Progress

### Complete Experiment Results (Qwen-2.5-3B, Constraint Satisfaction)

| Protocol | Success | 95% CI | Method | Status |
|----------|---------|--------|--------|--------|
| P0 | 0% | [0%, 16.1%] | No communication | Baseline |
| **P1** | **30%** | **[14.5%, 51.9%]** | **Text messages** | **Best** |
| E0 | 0% | [0%, 27.8%] | CIPHER embeddings | âŒ Failed |
| A0 L9 | 0% | [0%, 27.8%] | Activation (early layer) | âŒ Failed |
| A0 L18 | 0% | [0%, 27.8%] | Activation (mid layer) | âŒ Failed |
| A0 L27 | 0% | [0%, 27.8%] | Activation (late layer) | âŒ Failed |

**Key Findings:**
1. **P1 >> P0** confirmed - text communication helps (30% vs 0%)
2. **E0 = P0** - raw embeddings don't transfer semantics
3. **A0 = P0** - raw activations don't transfer semantics at ANY layer
4. **Conclusion:** Latent communication requires TRAINING to be effective
5. **Safety:** All E5 metrics passed pre-committed thresholds

### Analysis Capabilities
- **plot_results.py**: Success rates, capability vs bits, distributions
- **statistical_tests.py**: Hypothesis tests, pairwise comparisons, effect sizes

### Safety Infrastructure
- **ComplianceGapTester**: Monitored vs unmonitored behavior testing
- **LinearProbeMonitor**: Activation-based risk scoring
- **MonitorEnsemble**: Ensemble predictions with disagreement tracking
- **CovertChannelProbe**: Hidden channel capacity estimation
- **Stop Conditions**: All pre-committed thresholds implemented

### Experiment Infrastructure (New)
- **run_experiment.py**: Full-featured experiment runner with multi-protocol support
- **Experiment configs**: YAML configs for E1-E5 with inheritance support
- **Config loader**: Type-safe config loading with dataclass validation
- **Results aggregation**: Cross-experiment aggregation with LaTeX export

### Test Coverage
- **Channels:** 18 tests
- **Environments:** 22 tests
- **Metrics:** 18 tests
- **Integration:** 7 tests
- **Safety:** 28 tests
- **Config Loader:** 21 tests
- **Total:** 126 tests, 100% pass rate

---

## Code Structure

```
HDL/
â”œâ”€â”€ configs/                 # Experiment configurations
â”‚   â”œâ”€â”€ base.yaml           # Base configuration
â”‚   â”œâ”€â”€ e1_baseline.yaml    # E1: Text baseline validation
â”‚   â”œâ”€â”€ e2_task_sweep.yaml  # E2: Task family sweep
â”‚   â”œâ”€â”€ e3_cipher.yaml      # E3: CIPHER evaluation
â”‚   â”œâ”€â”€ e4_activation_grafting.yaml  # E4: Activation grafting
â”‚   â””â”€â”€ e5_safety.yaml      # E5: Safety evaluation
â”‚
â”œâ”€â”€ docs/                    # Planning documents
â”‚   â”œâ”€â”€ PLAN.md             # Master research plan
â”‚   â”œâ”€â”€ EXPERIMENTS.md      # Experimental protocols
â”‚   â”œâ”€â”€ METRICS.md          # Pre-registered metrics
â”‚   â”œâ”€â”€ BASELINES.md        # Baseline specifications
â”‚   â”œâ”€â”€ SAFETY_PROTOCOL.md  # Safety evaluation protocol
â”‚   â””â”€â”€ REPRODUCIBILITY.md  # Reproducibility checklist
â”‚
â”œâ”€â”€ lpca/                    # Main package
â”‚   â”œâ”€â”€ core/               # Infrastructure (complete)
â”‚   â”‚   â”œâ”€â”€ config_loader.py  # YAML config loading with inheritance
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ envs/               # Task environments (complete)
â”‚   â”œâ”€â”€ channels/           # Communication protocols (complete)
â”‚   â”œâ”€â”€ agents/             # Agent implementations (complete)
â”‚   â””â”€â”€ safety/             # Safety evaluation (complete)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_experiment.py   # Full experiment runner
â”‚   â”œâ”€â”€ demo_experiment.py  # End-to-end demo
â”‚   â””â”€â”€ analysis/           # Analysis scripts
â”‚       â”œâ”€â”€ plot_results.py
â”‚       â”œâ”€â”€ statistical_tests.py
â”‚       â””â”€â”€ aggregate_results.py  # Results aggregation
â”‚
â””â”€â”€ tests/                   # Test suite (126 tests)
```

---

## Implementation Metrics

| Metric | Value |
|--------|-------|
| Python files | 27 |
| Lines of code | ~7,000 |
| Test count | 126 |
| Test pass rate | 100% |
| Config files | 6 |
| Documentation pages | 7 |

---

## Immediate Next Steps

### âœ… COMPLETED
1. ~~Install torch/transformers~~ - Done (PyTorch 2.9.1, MPS support)
2. ~~Run E1 baseline validation~~ - Done (P1=30% >> P0=0%)
3. ~~Run E3 CIPHER evaluation~~ - Done (E0=0%, negative result)
4. ~~Run E4 layer sweep~~ - Done (A0=0% at L9, L18, L27)
5. ~~Run E5 safety evaluation~~ - Done (All metrics PASS)

### ðŸš€ NEXT: Milestone 2 (Requires Cloud GPUs)

**Decision Point:** Proceed to M2 codec training?
- **Rationale for YES:** M1 negative result is expected - untrained channels can't decode semantics. M2 will train codecs to make latent communication meaningful.
- **Rationale for NO:** Could be premature if task design is the issue, not channel training.

**If proceeding to M2:**
1. **Provision cloud GPU** - Lambda Labs A100 recommended (~$50-100)
2. **Collect P1 training data** - 1000+ successful episodes
3. **Train encoder-decoder** - Map text messages to latent packets
4. **Evaluate L1** - Compare trained latent vs P1 text

### Alternative: Scale Up E1 for Publication
- Run 100+ episodes for tighter confidence intervals
- Run on additional task types (arithmetic, program synthesis)
- Can be done locally on M3 Max

---

## Risk Status

| Risk | Status | Mitigation |
|------|--------|------------|
| Communication not bottleneck | Preliminary OK | P1 >> P0 in demo |
| MPS compatibility | Addressed | Lazy imports, fallbacks |
| Model loading | Ready | LLM agent implemented |
| Test coverage | Good | 77 tests, 100% pass |
| Safety monitoring | Ready | Full infrastructure |

---

## Repository

**GitHub:** https://github.com/MJ-Ref/HDL

**Latest Commits:**
- `8ccf654` Add analysis scripts, safety monitors, and update documentation
- `4f74c61` Fix bugs, add lazy imports, and comprehensive test suite
- `76fee53` Implement Milestone 1 latent communication channels

---

*Status updated: January 14, 2026*
